Long-Tailed Classification by Efficient Contrast Learning with High Quality and High Relevance Latent Features 
It is important to learn robust feature representations from long-tail distributed data. Recently, contrast learning has made remarkable achievements in long-tail learning, Contrastive learning can be seen as optimizing the lower bound of mutual information, but previous methods made inaccurate assumptions about model distribution and ignored the long tail problem of sample space, resulting in the lower bound not being tight enough. Besides, previous contrast learning methods made insufficient use of information to guide feature representation learning. In this paper, we first propose a loss function using batch sample features and class prototypes to construct a Conditional Gaussian mixture distribution (CGM-BF-CP), and prove its generalization ability from the perspective of generalization error upper bound. Then we create High Quality and High Relevance KNN graph to model relation between features and proposed a corresponding loss function, Graph based Contrast Learning Loss (GCLL). The feature information can be transferred between classes through this graph, so that the tail class features can be better learned. The experimental results on Cifar10/100-LT and ImageNet-LT show that our proposed model is competitive with the latest methods.
